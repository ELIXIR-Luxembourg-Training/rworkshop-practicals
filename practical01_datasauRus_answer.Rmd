---
title: "My answers"
author: "My name"
date: "2023-02-12"
output: html_document
---

```{r setup, include = FALSE}
library(tidyverse)
```


![](https://jumpingrivers.github.io/datasauRus/logo.png)


This guided practical will demonstrate that the **tidyverse** allows to compute summary statistics and visualize datasets efficiently.
This dataset is already stored in a **tidy** `tibble`, cleaning steps will come in future practicals.

##### Those kind of questions are optional {.bonus}

## `datasauRus` package

- check if you have the package `datasauRus` installed

```{r}
library(datasauRus)
```

- should return nothing. If `there is no package called ‘datasauRus’` appears, it means that the package needs 
to be installed. Use this:


## Explore the dataset

Since we are dealing with a `tibble`, we can type



Only the first **10** rows are displayed.

```{r}
datasaurus_dozen
```

##### What are the dimensions of this dataset? Rows and columns?

- **base** version, using either `dim()`, `ncol()` and `nrow()`

```{r}
# dim() returns the dimensions of the data frame, i.e number of rows and columns
dim(datasaurus_dozen)
# ncol() only number of columns
ncol(datasaurus_dozen)
# nrow() only number of rows
nrow(datasaurus_dozen)
```



- **tidyverse** version

```{r}
# Nothing to be done, a `tibble` display its dimensions, starting by a comment ('#' character)
```


##### Assign the `datasaurus_dozen` to the `ds_dozen` name. This aims at populating the **Global Environment**

```{r}
ds_dozen <- datasaurus_dozen
```




## How many datasets are present?


```{r}
# n_distinct counts the unique elements in a given vector.
# we use summarise to return only the desired column named n here.
# we use English verbs and no subsetting characters, nor we change dimensions (keep a tibble)
summarise(ds_dozen, n = n_distinct(dataset))
```


- Even better, compute and display the number of lines per `dataset`

```{block, opts.label = "tip"}
The function `count` in `dplyr` does the `group_by()` by the specified column + `summarise(n = n())` which returns the number of observation per defined group.
```

```{r}
count(ds_dozen, dataset)
```



## Check summary statistics per dataset

##### Compute the mean of the `x` & `y` column. For this, you need to `group_by()` the appropriate column and then `summarise()`

```{block, opts.label = "tip"}
In `summarise()` you can define as many new columns as you wish. No need to call it for every single variable.
```

```{r}
ds_dozen |>
  group_by(dataset) |>
  summarise(mean_x = mean(x),
            mean_y = mean(y))
```


##### Compute both mean and standard deviation (sd) in one go using `across()` {.bonus}


```{r}
ds_dozen |>
  # across works with first on which columns and second on what to perform on selection
  # 2 possibilities to select columns
  # summarise(across(where(is.double), list(mean = mean, sd = sd)))
  # by default since v1.0.5, grouped variables are excluded from across
  # summarise(across(everything(), list(mean = mean, sd = sd)))
  # we can use the new .by argument instead of a group_by()
  summarise(across(c(x, y), list(mean = mean, sd = sd)), .by = dataset)
```


Alternative of `across()` using [pivoting](https://tidyr.tidyverse.org/articles/pivot.html):

```{r}
ds_dozen |> 
  pivot_longer(cols = c(x, y),
               # to get all x first, then the y instead of x/y mingled
               cols_vary = "slowest",
               names_to = "variables",
               values_to = "values") |> 
  summarise(means = mean(values),
            sds = sd(values),
            .by = c(dataset, variables)) |> 
  print(n = Inf)
```



##### Compute the Pearson correlation between x and y per dataset?

```{r}
# pearson is cor() default but worth making it clear
summarise(ds_dozen, pearson_cor = cor(x, y, method = "pearson"), .by = dataset)
```




##### Perform a linear model of y explained by x per dataset

Correlation is easy enough as it returns a double and takes vectors as input.
For linear model, the R syntax `lm(y ~ x, data = dino)` makes it more complex to perform.

One elegant solution is to use [functional programming](https://purrr.tidyverse.org/reference/index.html) and [nesting](https://tidyr.tidyverse.org/articles/nest.html).
Combination with [`broom`](https://broom.tidymodels.org/) allows nice conversion of list  model output to rectangle `tibbles`.

```{r}
ds_dozen |>
  nest(.by = dataset) |> 
  mutate(lm = map(data, \(x) lm(x ~ y, data = x)),
         glance_lm = map(lm, broom::glance),
         r_squared = map_dbl(glance_lm, \(x) pull(x, r.squared)
         )) 
```




##### What can you conclude?

All mean, standard deviations and correlations are the same for the 13 datasets.
At least R^2 differ slightly.



## Plot the _datasauRus_

##### Plot the `ds_dozen` with `ggplot` such the **aesthetics** are `aes(x = x, y = y)` 
with the **geometry** `geom_point()`

```{block, opts.label = "tip"}
the `ggplot()` and `geom_point()` functions must be linked with a **+** sign
```

```{r}
ggplot(ds_dozen, aes(x = x, y = y)) +
  geom_point()
```


##### Reuse the above command, and now colored by the `dataset` column

```{r}
ggplot(ds_dozen, 
       aes(x = x, 
           y = y, 
           colour = dataset)) +
  geom_point()
```


Too many datasets are displayed.

##### Plot one `dataset` per **facet**

```{r}
ds_dozen |>
  ggplot(aes(x = x, y = y, colour = dataset)) +
  geom_point() +
  facet_wrap(~ dataset)
```


##### Tweak the theme and use the `theme_void()` and remove the legend

```{r}
ds_dozen |>
  ggplot(aes(x = x, y = y, colour = dataset)) +
  geom_point() +
  theme_void() +
  theme(legend.position = "none") +
  facet_wrap(~ dataset, ncol = 3)
```


##### Are the datasets actually that similar?

No ;) We were fooled by the summary stats



## Animation

Plots can be animated, see for example what can be done with [`gganimate`](https://gganimate.com/).
Instead of panels, **states** are made across `datasets` and **transitions** smoothed with an afterglow effect.
This takes around 1 minute to be generated, we will see in `targets` [demo](https://gitlab.lcsb.uni.lu/aurelien.ginolhac/targets_demos) how to use an efficient workflow manager to smartly cache expensive steps.

![](https://i.imgur.com/51GcnEp.png)


## `targets`, a workflow manager for R

[`targets`](https://github.com/ropensci/targets) is written by [William Landau](https://wlandau.github.io/about.html) and allows to create pipeline with connected dependencies depicted as follows:

![](https://gitlab.lcsb.uni.lu/aurelien.ginolhac/targets_demos/-/raw/main/img/dag_linear.png)

We will now present this along 3 examples of gradual complexity at this [repository for demos](https://gitlab.lcsb.uni.lu/aurelien.ginolhac/targets_demos). An optional lecture on Day4 will be presented for those who want to learn more about this.


## Conclusion

> Never trust summary statistics alone; always visualize your data | Alberto Cairo

**Authors**

- Alberto Cairo, (creator)
- Justin Matejka
- George Fitzmaurice
- Lucy McGowan

From this [post](https://itsalocke.com/datasaurus-now-cran/)
